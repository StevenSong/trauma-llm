# lightning.pytorch==2.5.0.post0
seed_everything: 42
trainer:
  # devices: 1
  # limit_train_batches: 16
  # limit_val_batches: 16
  # limit_test_batches: 16
  accumulate_grad_batches: 8 # check what batch size is
  precision: "bf16-mixed"
  logger:
    class_path: StrictWandbLogger
    init_args:
      project: "TraumaLLM"
      save_dir: "./runs"
  callbacks:
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: "val_loss"
        mode: "min"
        patience: 6
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_loss"
        mode: "min"
        save_last: "link"
  plugins:
    - class_path: lightning.pytorch.plugins.environments.SLURMEnvironment
      init_args:
        auto_requeue: False
  max_epochs: 100
  log_every_n_steps: 1
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    weight_decay: 0.01
lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    monitor: "val_loss"
    patience: 2
model:
  class_path: LightningClassifierModel
  init_args:
    backbone_name: "whaleloops/clinicalmamba-130m-hf"
    backbone_type: "mamba"
    backbone_dim: 768
data:
  class_path: LightningTraumaData
  init_args:
    data_dir: "/gpfs/data/benjamin-lab/Trauma_LLM/data"
    tokenizer_name: "whaleloops/clinicalmamba-130m-hf"
    context_length: 16384
    batch_size: 3 # check what grad accumulation is
    num_workers: 2
    split_seed: 42
    note_types:
    - "hp"
    - "op"
    - "tert"
